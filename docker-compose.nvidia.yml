version: '3.9'

services:
  llama:
    restart: always
    build:
      context: ${BUILD_CONTEXT}
      dockerfile: ${LLAMA_CUDA_DOCKERFILE}
      args:
        - NPROC=16
    ports:
      - "8000:8000"
    volumes:
      - models:/app/models
    command: ${LLAMA_ARGS}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - populate-models

  sd:
    restart: always
    build:
      context: ${BUILD_CONTEXT}
      dockerfile: ${SD_CUDA_DOCKERFILE}
    ports:
      - "8001:8001"
    volumes:
      - models:/app/models
    command: ${SD_ARGS}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - populate-models

  nginx:
    build:
      context: ./nginx
      args:
        NGINX_SECRET: ${NGINX_SECRET}
    restart: always
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - llama
      - sd
    environment:
      - NGINX_SECRET=${NGINX_SECRET}

  populate-models:
    image: alpine
    volumes:
      - ${LLAMA_MODEL_PATH}:/source/llama
      - ${SD_MODEL_PATH}:/source/sd
      - models:/models
    command: sh -c "rm -rf /models/* && cp -av /source/llama/. /models && cp -av /source/sd/. /models"


volumes:
  models: