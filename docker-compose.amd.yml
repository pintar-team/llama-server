services:
  llama:
    profiles: ["llama"]
    restart: always
    build:
      context: ${BUILD_CONTEXT}
      dockerfile: ${LLAMA_VK_DOCKERFILE}
      args:
        - NPROC=16
    ports:
      - "8000:8000"
    volumes:
      - "${LLAMA_MODEL_PATH}:/app/models"
    command: ${LLAMA_ARGS}
    devices:
      - /dev/fdk
      - /dev/dri
    security_opt:
      - seccomp:unconfined

  sd:
    profiles: ["sd"]
    restart: always
    build:
      context: ${BUILD_CONTEXT}
      dockerfile: ${SD_VK_DOCKERFILE}
    ports:
      - "8001:8001"
    volumes:
      - "${SD_MODEL_PATH}:/app/models"
    command: ${SD_ARGS}
    devices:
      - /dev/fdk
      - /dev/dri
    security_opt:
      - seccomp:unconfined

  nginx:
    build: 
      context: ./nginx
      args:
        NGINX_SECRET: ${NGINX_SECRET}
        ENABLE_LLAMA: ${ENABLE_LLAMA}
        ENABLE_SD: ${ENABLE_SD}
    restart: always
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - "${ACTIVE_PROFILES}"
    environment:
      - NGINX_SECRET=${NGINX_SECRET}
      - ENABLE_LLAMA=${ENABLE_LLAMA}
      - ENABLE_SD=${ENABLE_SD}
    profiles:
      - llama
      - sd
