version: '3'

services:
  llama:
    build:
      context: ./llama-cuda
      args:
        - NPROC=8
    ports:
      - "8000:8000"
    volumes:
      - ./llama.cpp:/app
    working_dir: /app

  nginx:
    build: ./nginx
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - llama
